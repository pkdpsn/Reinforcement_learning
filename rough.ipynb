{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete,Box \n",
    "import numpy as np\n",
    "import random\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "import pygame\n",
    "white = (255, 255, 255)\n",
    "colors = [\n",
    "    (0, 0, 255),  # Blue\n",
    "    (0, 255, 0),  # Green\n",
    "    (255, 0, 0),  # Red\n",
    "    (255, 255, 0),  # Yellow\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (0, 255, 255),  # Cyan\n",
    "    (128, 0, 0),  # Maroon\n",
    "    (0, 128, 0),  # Olive\n",
    "    (128, 128, 0),  # Yellow Green\n",
    "    (0, 0, 128),  # Navy\n",
    "    (128, 0, 128),  # Purple\n",
    "    (0, 128, 128),  # Teal\n",
    "    (128, 128, 128),  # Gray\n",
    "    (255, 165, 0)  # Orange (for special cell)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class rlEnv(Env):\n",
    "    \n",
    "    def __init__(self, W, H):\n",
    "        super(rlEnv, self).__init__()\n",
    "        self.W = W\n",
    "        self.H = H\n",
    "        self.stepsize = (2 / self.W)\n",
    "        self.gravity = 9.81\n",
    "        self.truncated = False\n",
    "        self.done = False\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(W * H)\n",
    "        self.grid = np.zeros((H, W))\n",
    "        for i in range(W):\n",
    "            for j in range(H):\n",
    "                x = -3 + i * (6 / self.W)\n",
    "                y = -3 + j * (6 / self.H)\n",
    "                self.grid[j, i] = 16 * 0.4 * (x**2 + y**2 - 1/4)**2 if x**2 + y**2 < 1/4 else 0\n",
    "\n",
    "        self.screen_width = 800\n",
    "        self.screen_height = 800\n",
    "        self.screen = None\n",
    "        self.state = self._to_s(int((self.H) / 2), int(0.25 * (self.W)))\n",
    "        self.velocity = 1\n",
    "        self.reward = 0\n",
    "        self.collective = 0\n",
    "        \n",
    "    def _to_s(self, row, col):\n",
    "        return row * self.W + col\n",
    "\n",
    "    def step(self, action):\n",
    "        row, col = divmod(self.state, self.W)\n",
    "        prev_row, prev_col = row, col\n",
    "        h_prev = self.grid[row, col]\n",
    "\n",
    "        if self.velocity == 0 or self.collective < -30:  # Check termination conditions\n",
    "            self.reward = -10\n",
    "            self.truncated = True\n",
    "        elif (row == int((self.H) / 2) and col == int(0.75 * self.W)):\n",
    "            self.reward = 0\n",
    "            self.done = True\n",
    "        else:\n",
    "            self.done = False\n",
    "            if action == 0:  # Move left\n",
    "                col = max(col - 1, 0)\n",
    "            elif action == 1:  # Move down\n",
    "                row = min(row + 1, self.H - 1)\n",
    "            elif action == 2:  # Move right\n",
    "                col = min(col + 1, self.W - 1)\n",
    "            elif action == 3:  # Move up\n",
    "                row = max(row - 1, 0)\n",
    "\n",
    "            if ((prev_col - col + prev_row - row) == 0):\n",
    "                self.reward = -1\n",
    "            else:\n",
    "                self.reward = -float(self.stepsize / self.velocity)\n",
    "\n",
    "            h_new = self.grid[row, col]\n",
    "            update_vel = self.velocity ** 2 + 2 * self.gravity * (h_prev - h_new)\n",
    "            if update_vel < 0:\n",
    "                self.velocity = 0\n",
    "            else:\n",
    "                self.velocity = sqrt(update_vel)\n",
    "            \n",
    "            self.state = self._to_s(row, col)\n",
    "            self.collective += self.reward\n",
    "\n",
    "        return self.state, self.reward, self.done, self.truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "            self.screen.fill((255, 255, 255))\n",
    "\n",
    "            for i in range(self.W):\n",
    "                for j in range(self.H):\n",
    "                    x = i * (self.screen_width / self.W)\n",
    "                    y = j * (self.screen_height / self.H)\n",
    "                    color_index = int(self.grid[i, j] * 255)\n",
    "                    color = (255 - color_index, 0, color_index)\n",
    "                    pygame.draw.rect(self.screen, color, (x, y, self.screen_width / self.W, self.screen_height / self.H))\n",
    "            \n",
    "            agent_row, agent_col = divmod(self.state, self.W)\n",
    "            agent_x = agent_col * (self.screen_width / self.W)\n",
    "            agent_y = agent_row * (self.screen_height / self.H) + (self.screen_height / self.H) / 2\n",
    "            pygame.draw.circle(self.screen, (0, 0, 0), (int(agent_x), int(agent_y)), 10)\n",
    "            pygame.display.flip()\n",
    "        \n",
    "        else:\n",
    "            agent_row, agent_col = divmod(self.state, self.W)\n",
    "            agent_x = agent_col * (self.screen_width / self.W)\n",
    "            agent_y = agent_row * (self.screen_height / self.H) + (self.screen_height / self.H) / 2\n",
    "            pygame.draw.circle(self.screen, (0, 0, 0), (int(agent_x), int(agent_y)), 10)\n",
    "            pygame.display.flip()\n",
    "\n",
    "        return\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            pygame.quit()\n",
    "            self.screen = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self._to_s(int((self.H) / 2), int(0.25 * (self.W)))\n",
    "        self.velocity = 1\n",
    "        self.reward = 0\n",
    "        self.truncated = False\n",
    "        self.done = False\n",
    "        self.collective = 0\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=rlEnv(20,20)\n",
    "env.render()\n",
    "pygame.time.wait(1000)\n",
    "print(env.screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "SNAKE_LEN_GOAL = 30\n",
    "\n",
    "def collision_with_apple(apple_position, score):\n",
    "\tapple_position = [random.randrange(1,50)*10,random.randrange(1,50)*10]\n",
    "\tscore += 1\n",
    "\treturn apple_position, score\n",
    "\n",
    "def collision_with_boundaries(snake_head):\n",
    "\tif snake_head[0]>=500 or snake_head[0]<0 or snake_head[1]>=500 or snake_head[1]<0 :\n",
    "\t\treturn 1\n",
    "\telse:\n",
    "\t\treturn 0\n",
    "\n",
    "def collision_with_self(snake_position):\n",
    "\tsnake_head = snake_position[0]\n",
    "\tif snake_head in snake_position[1:]:\n",
    "\t\treturn 1\n",
    "\telse:\n",
    "\t\treturn 0\n",
    "\n",
    "\n",
    "class SnekEnv(gym.Env):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(SnekEnv, self).__init__()\n",
    "\t\t# Define action and observation space\n",
    "\t\t# They must be gym.spaces objects\n",
    "\t\t# Example when using discrete actions:\n",
    "\t\tself.action_space = spaces.Discrete(4)\n",
    "\t\t# Example for using image as input (channel-first; channel-last also works):\n",
    "\t\tself.observation_space = spaces.Box(low=-500, high=500,\n",
    "\t\t\t\t\t\t\t\t\t\t\tshape=(5+SNAKE_LEN_GOAL,), dtype=np.float32)\n",
    "\n",
    "\tdef step(self, action):\n",
    "\t\tself.prev_actions.append(action)\n",
    "\t\tcv2.imshow('a',self.img)\n",
    "\t\tcv2.waitKey(1)\n",
    "\t\tself.img = np.zeros((500,500,3),dtype='uint8')\n",
    "\t\t# Display Apple\n",
    "\t\tcv2.rectangle(self.img,(self.apple_position[0],self.apple_position[1]),(self.apple_position[0]+10,self.apple_position[1]+10),(0,0,255),3)\n",
    "\t\t# Display Snake\n",
    "\t\tfor position in self.snake_position:\n",
    "\t\t\tcv2.rectangle(self.img,(position[0],position[1]),(position[0]+10,position[1]+10),(0,255,0),3)\n",
    "\t\t\n",
    "\t\t# Takes step after fixed time\n",
    "\t\tt_end = time.time() + 0.05\n",
    "\t\tk = -1\n",
    "\t\twhile time.time() < t_end:\n",
    "\t\t\tif k == -1:\n",
    "\t\t\t\tk = cv2.waitKey(1)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\tbutton_direction = action\n",
    "\t\t# Change the head position based on the button direction\n",
    "\t\tif button_direction == 1:\n",
    "\t\t\tself.snake_head[0] += 10\n",
    "\t\telif button_direction == 0:\n",
    "\t\t\tself.snake_head[0] -= 10\n",
    "\t\telif button_direction == 2:\n",
    "\t\t\tself.snake_head[1] += 10\n",
    "\t\telif button_direction == 3:\n",
    "\t\t\tself.snake_head[1] -= 10\n",
    "\n",
    "\t\t# Increase Snake length on eating apple\n",
    "\t\tif self.snake_head == self.apple_position:\n",
    "\t\t\tself.apple_position, self.score = collision_with_apple(self.apple_position, self.score)\n",
    "\t\t\tself.snake_position.insert(0,list(self.snake_head))\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tself.snake_position.insert(0,list(self.snake_head))\n",
    "\t\t\tself.snake_position.pop()\n",
    "\t\t\n",
    "\t\t# On collision kill the snake and print the score\n",
    "\t\tif collision_with_boundaries(self.snake_head) == 1 or collision_with_self(self.snake_position) == 1:\n",
    "\t\t\tfont = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\t\t\tself.img = np.zeros((500,500,3),dtype='uint8')\n",
    "\t\t\tcv2.putText(self.img,'Your Score is {}'.format(self.score),(140,250), font, 1,(255,255,255),2,cv2.LINE_AA)\n",
    "\t\t\tcv2.imshow('a',self.img)\n",
    "\t\t\tself.done = True\n",
    "\n",
    "\t\tself.total_reward = len(self.snake_position) - 3  # default length is 3\n",
    "\t\tself.reward = self.total_reward - self.prev_reward\n",
    "\t\tself.prev_reward = self.total_reward\n",
    "\n",
    "\t\tif self.done:\n",
    "\t\t\tself.reward = -10\n",
    "\t\tinfo = {}\n",
    "\n",
    "\n",
    "\t\thead_x = self.snake_head[0]\n",
    "\t\thead_y = self.snake_head[1]\n",
    "\n",
    "\t\tsnake_length = len(self.snake_position)\n",
    "\t\tapple_delta_x = self.apple_position[0] - head_x\n",
    "\t\tapple_delta_y = self.apple_position[1] - head_y\n",
    "\n",
    "\t\t# create observation:\n",
    "\n",
    "\t\tobservation = [head_x, head_y, apple_delta_x, apple_delta_y, snake_length] + list(self.prev_actions)\n",
    "\t\tobservation = np.array(observation)\n",
    "\n",
    "\t\treturn observation, self.reward, self.done, info\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.img = np.zeros((500,500,3),dtype='uint8')\n",
    "\t\t# Initial Snake and Apple position\n",
    "\t\tself.snake_position = [[250,250],[240,250],[230,250]]\n",
    "\t\tself.apple_position = [random.randrange(1,50)*10,random.randrange(1,50)*10]\n",
    "\t\tself.score = 0\n",
    "\t\tself.prev_button_direction = 1\n",
    "\t\tself.button_direction = 1\n",
    "\t\tself.snake_head = [250,250]\n",
    "\n",
    "\t\tself.prev_reward = 0\n",
    "\n",
    "\t\tself.done = False\n",
    "\n",
    "\t\thead_x = self.snake_head[0]\n",
    "\t\thead_y = self.snake_head[1]\n",
    "\n",
    "\t\tsnake_length = len(self.snake_position)\n",
    "\t\tapple_delta_x = self.apple_position[0] - head_x\n",
    "\t\tapple_delta_y = self.apple_position[1] - head_y\n",
    "\n",
    "\t\tself.prev_actions = deque(maxlen = SNAKE_LEN_GOAL)  # however long we aspire the snake to be\n",
    "\t\tfor i in range(SNAKE_LEN_GOAL):\n",
    "\t\t\tself.prev_actions.append(-1) # to create history\n",
    "\n",
    "\t\t# create observation:\n",
    "\t\tobservation = [head_x, head_y, apple_delta_x, apple_delta_y, snake_length] + list(self.prev_actions)\n",
    "\t\tobservation = np.array(observation)\n",
    "\n",
    "\t\treturn observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "\n",
    "env = SnekEnv()\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "# from snakeenv import SnekEnv\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "models_dir = f\"models/{int(time.time())}/\"\n",
    "logdir = f\"logs/{int(time.time())}/\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "\tos.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "\tos.makedirs(logdir)\n",
    "\n",
    "env = SnekEnv()\n",
    "env.reset()\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=logdir)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 0\n",
    "while True:\n",
    "\titers += 1\n",
    "\tmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO\")\n",
    "\tmodel.save(f\"{models_dir}/{TIMESTEPS*iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO,DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "def make_env(env_id: str, rank: int, seed: int = 0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: the environment ID\n",
    "    :param num_env: the number of environments you wish to have in subprocesses\n",
    "    :param seed: the inital seed for RNG\n",
    "    :param rank: index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = gym.make(env_id)\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_id = \"CartPole-v1\"\n",
    "    num_cpu = 1# Number of processes to use\n",
    "    # Create the vectorized environment\n",
    "    vec_env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])\n",
    "\n",
    "    # Stable Baselines provides you with make_vec_env() helper\n",
    "    # which does exactly the previous steps for you.\n",
    "    # You can choose between `DummyVecEnv` (usually faster) and `SubprocVecEnv`\n",
    "    # env = make_vec_env(env_id, n_envs=num_cpu, seed=0, vec_env_cls=SubprocVecEnv)\n",
    "\n",
    "    model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
    "    model.learn(total_timesteps=25_000)\n",
    "\n",
    "    obs = vec_env.reset()\n",
    "    for _ in range(1000):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = vec_env.step(action)\n",
    "        vec_env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
